{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hwangho-kim/Utility-OAC/blob/main/Daily_FDC_Monitoring_with_Simplified_Isolation_Forest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import IsolationForest\n",
        "import joblib\n",
        "from datetime import date, timedelta\n",
        "\n",
        "# --- 그래프 한글 폰트 설정 ---\n",
        "try:\n",
        "    import koreanize_matplotlib\n",
        "except ImportError:\n",
        "    pass\n",
        "\n",
        "# --- 데이터 생성 및 특징 추출 함수들 (이전과 동일) ---\n",
        "def create_long_format_csv(filepath, num_wafers, anomaly_info=None):\n",
        "    print(f\"'{filepath}' 이름으로 샘플 CSV 파일을 생성합니다...\")\n",
        "    num_steps, num_sensors, time_points = 10, 10, 10\n",
        "    np.random.seed(42)\n",
        "    records = []\n",
        "    for wafer_id in range(1, num_wafers + 1):\n",
        "        for step_id in range(1, num_steps + 1):\n",
        "            record_base = {'wafer_id': wafer_id, 'step_id': step_id}\n",
        "            for i in range(num_sensors):\n",
        "                sensor_name = f'Sensor_{chr(65+i)}'\n",
        "                record_base[sensor_name] = np.random.randn(time_points) * (i * 0.1 + 0.5) + (i * 5 + step_id)\n",
        "            if anomaly_info:\n",
        "                if (anomaly_info.get('wafer_id') is None or wafer_id >= anomaly_info['wafer_id']) and \\\n",
        "                   (anomaly_info.get('step_id') is None or step_id == anomaly_info['step_id']):\n",
        "                    record_base['Sensor_C'] += anomaly_info.get('c_drift', 0)\n",
        "                    record_base['Sensor_G'] *= anomaly_info.get('g_noise', 1)\n",
        "            for t_idx in range(time_points):\n",
        "                row = {**record_base, 'time': t_idx}\n",
        "                for i in range(num_sensors):\n",
        "                    sensor_name = f'Sensor_{chr(65+i)}'\n",
        "                    row[sensor_name] = record_base[sensor_name][t_idx]\n",
        "                records.append(row)\n",
        "    df = pd.DataFrame(records)\n",
        "    df.to_csv(filepath, index=False)\n",
        "    print(\"샘플 파일 생성 완료.\")\n",
        "    return df\n",
        "\n",
        "def find_columns(df):\n",
        "    id_candidates = {'wafer': ['wafer_id'], 'step': ['step_id'], 'time': ['time']}\n",
        "    detected_cols = {}\n",
        "    remaining_cols = list(df.columns)\n",
        "    for id_type, candidates in id_candidates.items():\n",
        "        found = False\n",
        "        for col in remaining_cols:\n",
        "            if col.lower() in candidates:\n",
        "                detected_cols[id_type] = col; remaining_cols.remove(col); found = True; break\n",
        "        if not found: detected_cols[id_type] = None\n",
        "    sensor_cols = [col for col in remaining_cols if pd.api.types.is_numeric_dtype(df[col])]\n",
        "    return detected_cols['wafer'], detected_cols['step'], sensor_cols\n",
        "\n",
        "def extract_and_pivot_features(df, wafer_id_col, step_id_col, sensor_cols):\n",
        "    def rms(x): return np.sqrt(np.mean(x**2))\n",
        "    agg_funcs = ['mean', 'std', 'max', 'min', 'median', 'skew', pd.Series.kurt, rms]\n",
        "    features = df.groupby([wafer_id_col, step_id_col])[sensor_cols].agg(agg_funcs).reset_index()\n",
        "    new_cols = [wafer_id_col, step_id_col]\n",
        "    for col_level0, col_level1 in features.columns[2:]:\n",
        "        func_name = col_level1 if isinstance(col_level1, str) else col_level1.__name__\n",
        "        new_cols.append(f\"{col_level0}_{func_name}\")\n",
        "    features.columns = new_cols\n",
        "    feature_pivot = features.pivot(index=wafer_id_col, columns=step_id_col)\n",
        "    feature_pivot.columns = [f\"S{int(col[1])}_{col[0]}\" for col in feature_pivot.columns.values]\n",
        "    feature_pivot.fillna(0, inplace=True)\n",
        "    return feature_pivot.reset_index()\n",
        "\n",
        "# --- Wafer 진단 리포트 함수 ---\n",
        "def diagnose_top_wafers(df_daily, normal_stats, wafer_id_col, feature_cols, top_n=3):\n",
        "    print(f\"\\n--- 당일 Health Index 상위 {top_n}개 Wafer 상세 진단 ---\")\n",
        "    df_sorted = df_daily.sort_values(by='health_index', ascending=False)\n",
        "    for i, row in enumerate(df_sorted.head(top_n).itertuples()):\n",
        "        wafer_id = getattr(row, wafer_id_col)\n",
        "        z_scores = {}\n",
        "        for feature in feature_cols:\n",
        "            val = getattr(row, feature)\n",
        "            mean = normal_stats.loc['mean', feature]\n",
        "            std = normal_stats.loc['std', feature]\n",
        "            if std > 1e-6:\n",
        "                z_scores[feature] = (val - mean) / std\n",
        "        sorted_features = sorted(z_scores.items(), key=lambda item: abs(item[1]), reverse=True)\n",
        "        print(f\"\\n[{i+1}] Wafer ID: {wafer_id} (Health Index: {row.health_index:.4f})\")\n",
        "        print(\"  > 상위 원인 특징 (Z-score 기준):\")\n",
        "        for feature, score in sorted_features[:3]:\n",
        "            parts = feature.split('_')\n",
        "            step_info, stat_info = parts[0].replace('S', ''), parts[-1]\n",
        "            sensor_info = '_'.join(parts[1:-1])\n",
        "            print(f\"    - Step: {step_info}, Sensor: {sensor_info}, Statistic: {stat_info}, Z-score: {score:.2f}\")\n",
        "\n",
        "# ======================================================================================\n",
        "# Phase 1: 기준 모델 생성 (최초 1회 실행)\n",
        "# ======================================================================================\n",
        "def train_and_save_reference_model(golden_data_path='golden_data.csv'):\n",
        "    print(\"\\n\" + \"=\"*25 + \" Phase 1: 기준 모델 생성 (Simplified Isolation Forest) \" + \"=\"*25)\n",
        "\n",
        "    create_long_format_csv(golden_data_path, num_wafers=100)\n",
        "    df_long = pd.read_csv(golden_data_path)\n",
        "    wafer_id_col, step_id_col, sensor_cols = find_columns(df_long)\n",
        "    df_wide = extract_and_pivot_features(df_long, wafer_id_col, step_id_col, sensor_cols)\n",
        "    feature_cols = [col for col in df_wide.columns if col != wafer_id_col]\n",
        "\n",
        "    scaler_final = StandardScaler()\n",
        "    golden_features_scaled = scaler_final.fit_transform(df_wide[feature_cols])\n",
        "\n",
        "    iforest = IsolationForest(contamination='auto', random_state=42)\n",
        "    print(\"\\n아이솔레이션 포레스트 모델 학습 시작...\")\n",
        "    iforest.fit(golden_features_scaled)\n",
        "    print(\"모델 학습 완료.\")\n",
        "\n",
        "    # --- [수정됨] 가장 좋은 점수를 0점의 기준으로 설정 ---\n",
        "    decision_scores = iforest.decision_function(golden_features_scaled)\n",
        "    max_score_baseline = np.max(decision_scores)\n",
        "    print(f\"\\n가장 좋은 상태의 기준 점수(0점 기준): {max_score_baseline:.6f}\")\n",
        "\n",
        "    # 자가 진단: 골든 데이터셋의 평균 인덱스 출력\n",
        "    raw_health_indexes = max_score_baseline - decision_scores\n",
        "    print(f\"[자가 진단] 골든 데이터셋 자체의 평균 Health Index: {np.mean(raw_health_indexes):.4f}\")\n",
        "\n",
        "    normal_stats = df_wide[feature_cols].agg(['mean', 'std'])\n",
        "\n",
        "    # --- [수정됨] 저장할 모델 및 기준값 단순화 ---\n",
        "    joblib.dump(iforest, 'iforest_model.joblib')\n",
        "    joblib.dump(scaler_final, 'scaler_final.joblib')\n",
        "    joblib.dump(feature_cols, 'feature_cols.joblib')\n",
        "    joblib.dump(max_score_baseline, 'max_score_baseline.joblib')\n",
        "    joblib.dump(normal_stats, 'normal_stats.joblib')\n",
        "\n",
        "    print(\"\\n기준 모델 생성 및 저장 완료.\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "# ======================================================================================\n",
        "# Phase 2: 일일 모니터링 (매일 반복 실행)\n",
        "# ======================================================================================\n",
        "def run_daily_monitoring(days_to_monitor=10):\n",
        "    print(\"\\n\" + \"=\"*25 + \" Phase 2: 일일 모니터링 시작 \" + \"=\"*25)\n",
        "\n",
        "    try:\n",
        "        iforest = joblib.load('iforest_model.joblib')\n",
        "        scaler_final = joblib.load('scaler_final.joblib')\n",
        "        feature_cols = joblib.load('feature_cols.joblib')\n",
        "        max_score_baseline = joblib.load('max_score_baseline.joblib')\n",
        "        normal_stats = joblib.load('normal_stats.joblib')\n",
        "        print(\"저장된 기준 모델을 성공적으로 불러왔습니다.\")\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"오류: 기준 모델 파일(.joblib)을 찾을 수 없습니다. Phase 1을 먼저 실행해주세요. ({e})\")\n",
        "        return\n",
        "\n",
        "    daily_log = []\n",
        "    start_date = date.today()\n",
        "\n",
        "    for day in range(days_to_monitor):\n",
        "        current_date = start_date + timedelta(days=day)\n",
        "        print(f\"\\n--- {current_date} 데이터 모니터링 ---\")\n",
        "\n",
        "        daily_data_path = f'daily_data_day_{day+1}.csv'\n",
        "        degradation_info = {'c_drift': day * 0.05, 'g_noise': 1 + day * 0.01}\n",
        "        create_long_format_csv(daily_data_path, num_wafers=20, anomaly_info=degradation_info)\n",
        "        df_long_daily = pd.read_csv(daily_data_path)\n",
        "\n",
        "        wafer_id_col, step_id_col, sensor_cols = find_columns(df_long_daily)\n",
        "        df_wide_daily = extract_and_pivot_features(df_long_daily, wafer_id_col, step_id_col, sensor_cols)\n",
        "        df_wide_daily = df_wide_daily[df_wide_daily.columns.intersection([wafer_id_col] + feature_cols)]\n",
        "\n",
        "        # --- [수정됨] Health Index 계산 단순화 ---\n",
        "        daily_features_scaled = scaler_final.transform(df_wide_daily[feature_cols])\n",
        "\n",
        "        decision_scores = iforest.decision_function(daily_features_scaled)\n",
        "        raw_health_indexes = max_score_baseline - decision_scores\n",
        "        df_wide_daily['health_index'] = raw_health_indexes\n",
        "\n",
        "        daily_index = np.mean(raw_health_indexes)\n",
        "\n",
        "        print(f\"{current_date}의 Daily Index: {daily_index:.4f}\")\n",
        "        daily_log.append({'date': current_date, 'daily_index': daily_index})\n",
        "\n",
        "        diagnose_top_wafers(df_wide_daily, normal_stats, wafer_id_col, feature_cols, top_n=3)\n",
        "\n",
        "    log_df = pd.DataFrame(daily_log)\n",
        "    log_df.to_csv('daily_health_log.csv', index=False)\n",
        "    print(\"\\n일일 모니터링 로그 저장 완료: daily_health_log.csv\")\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    sns.lineplot(x='date', y='daily_index', data=log_df, marker='o')\n",
        "    plt.title('Daily Equipment Health Index Trend (Simplified)', fontsize=16)\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Daily Health Index (Lower is Better)')\n",
        "    plt.grid(True, linestyle='--')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    train_and_save_reference_model()\n",
        "    run_daily_monitoring(days_to_monitor=10)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "dCJ1cteYb2Y9"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}