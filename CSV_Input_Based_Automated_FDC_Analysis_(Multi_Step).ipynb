{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hwangho-kim/Utility-OAC/blob/main/CSV_Input_Based_Automated_FDC_Analysis_(Multi_Step).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "# --- 그래프 한글 폰트 설정 ---\n",
        "try:\n",
        "    import koreanize_matplotlib\n",
        "except ImportError:\n",
        "    pass\n",
        "\n",
        "def create_sample_long_format_csv(filepath='sample_fdc_long_data.csv'):\n",
        "    \"\"\"\n",
        "    [개선됨] 분석을 시연하기 위한 대규모 샘플 'Long Format' CSV 파일을 생성합니다.\n",
        "    - Wafer 수: 100개\n",
        "    - Step 수: 10개\n",
        "    - Sensor 수: 10개\n",
        "    \"\"\"\n",
        "    print(f\"'{filepath}' 이름으로 데모용 샘플 CSV 파일을 생성합니다.\")\n",
        "    num_wafers = 100\n",
        "    num_steps = 10\n",
        "    num_sensors = 10\n",
        "    time_points = 10\n",
        "    anomaly_start_wafer = 80\n",
        "    np.random.seed(42)\n",
        "\n",
        "    records = []\n",
        "    for wafer_id in range(1, num_wafers + 1):\n",
        "        for step_id in range(1, num_steps + 1):\n",
        "            record_base = {'wafer_id': wafer_id, 'step_id': step_id}\n",
        "\n",
        "            # 10개의 센서 데이터 생성\n",
        "            for i in range(num_sensors):\n",
        "                sensor_name = f'Sensor_{chr(65+i)}'\n",
        "                record_base[sensor_name] = np.random.randn(time_points) * (i * 0.1 + 0.5) + (i * 5 + step_id)\n",
        "\n",
        "            # 이상 상태 주입 (80번 Wafer부터, 5번 Step에서)\n",
        "            if wafer_id >= anomaly_start_wafer and step_id == 5:\n",
        "                record_base['Sensor_C'] += 3.0  # Sensor C의 평균이 증가\n",
        "                record_base['Sensor_G'] *= 1.5 # Sensor G의 변동성(분산)이 증가\n",
        "\n",
        "            for t_idx in range(time_points):\n",
        "                row = {\n",
        "                    'wafer_id': record_base['wafer_id'],\n",
        "                    'step_id': record_base['step_id'],\n",
        "                    'time': t_idx\n",
        "                }\n",
        "                for i in range(num_sensors):\n",
        "                    sensor_name = f'Sensor_{chr(65+i)}'\n",
        "                    row[sensor_name] = record_base[sensor_name][t_idx]\n",
        "                records.append(row)\n",
        "\n",
        "    df = pd.DataFrame(records)\n",
        "    df.to_csv(filepath, index=False)\n",
        "    print(\"샘플 파일 생성 완료.\")\n",
        "    return df\n",
        "\n",
        "def find_columns(df):\n",
        "    \"\"\"\n",
        "    DataFrame에서 ID 및 센서 컬럼들을 자동으로 탐지합니다.\n",
        "    \"\"\"\n",
        "    id_candidates = {\n",
        "        'wafer': ['wafer_id', 'waferid', 'wafer', 'wfr_id', 'wafer_no', 'lot_wafer'],\n",
        "        'step': ['step_id', 'stepid', 'step', 'step_no'],\n",
        "        'time': ['time', 'timestamp', 'time_sec']\n",
        "    }\n",
        "\n",
        "    detected_cols = {}\n",
        "    remaining_cols = list(df.columns)\n",
        "\n",
        "    for id_type, candidates in id_candidates.items():\n",
        "        found = False\n",
        "        for col in remaining_cols:\n",
        "            if col.lower() in candidates:\n",
        "                detected_cols[id_type] = col\n",
        "                remaining_cols.remove(col)\n",
        "                found = True\n",
        "                break\n",
        "        if not found:\n",
        "            print(f\"경고: '{id_type}' ID 컬럼을 찾지 못했습니다.\")\n",
        "            detected_cols[id_type] = None\n",
        "\n",
        "    sensor_cols = [col for col in remaining_cols if pd.api.types.is_numeric_dtype(df[col])]\n",
        "\n",
        "    return detected_cols['wafer'], detected_cols['step'], sensor_cols\n",
        "\n",
        "def extract_and_pivot_features(df, wafer_id_col, step_id_col, sensor_cols):\n",
        "    \"\"\"\n",
        "    Long format 데이터에서 통계 특징을 추출하고, Wafer 단위의 Wide format으로 변환합니다.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Step 1-2: 특징 추출 및 데이터 재구조화(Pivot) ---\")\n",
        "\n",
        "    features = df.groupby([wafer_id_col, step_id_col])[sensor_cols].agg(['mean', 'std', 'max', 'min']).reset_index()\n",
        "    features.columns = ['_'.join(col).strip() if isinstance(col, tuple) and col[1] != '' else col[0] for col in features.columns.values]\n",
        "\n",
        "    feature_pivot = features.pivot(index=wafer_id_col, columns=step_id_col)\n",
        "    feature_pivot.columns = [f\"S{int(col[1])}_{col[0]}\" for col in feature_pivot.columns.values]\n",
        "    feature_pivot.fillna(0, inplace=True)\n",
        "\n",
        "    print(f\"최종 특징 벡터 생성 완료. Shape: {feature_pivot.shape}\")\n",
        "    return feature_pivot.reset_index()\n",
        "\n",
        "def diagnose_top_wafers_by_health_index(df, golden_df, wafer_id_col, feature_cols, top_n=3):\n",
        "    \"\"\"\n",
        "    Health Index가 높은 상위 Wafer들에 대해 원인 특징을 진단합니다.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Step 5: Health Index 상위 Wafer 원인 분석 ---\")\n",
        "\n",
        "    if golden_df.empty:\n",
        "        print(\"정상 군집 데이터가 없어 원인 분석을 생략합니다.\")\n",
        "        return\n",
        "\n",
        "    normal_stats = golden_df[feature_cols].agg(['mean', 'std'])\n",
        "    df_sorted = df.sort_values(by='health_index', ascending=False)\n",
        "\n",
        "    print(f\"Health Index 기준 상위 {top_n}개 Wafer 진단 결과:\")\n",
        "\n",
        "    for i, row in enumerate(df_sorted.head(top_n).itertuples()):\n",
        "        wafer_id = getattr(row, wafer_id_col)\n",
        "        z_scores = {}\n",
        "        for feature in feature_cols:\n",
        "            val = getattr(row, feature)\n",
        "            mean = normal_stats.loc['mean', feature]\n",
        "            std = normal_stats.loc['std', feature]\n",
        "            if std > 1e-6:\n",
        "                z_scores[feature] = (val - mean) / std\n",
        "\n",
        "        sorted_features = sorted(z_scores.items(), key=lambda item: abs(item[1]), reverse=True)\n",
        "\n",
        "        print(f\"\\n[{i+1}] Wafer ID: {wafer_id} (Health Index: {row.health_index:.2f})\")\n",
        "        print(\"  > 상위 원인 특징 (Z-score 기준):\")\n",
        "        for feature, score in sorted_features[:3]:\n",
        "            print(f\"    - {feature}: {score:.2f}\")\n",
        "\n",
        "def analyze_fdc_from_csv(filepath):\n",
        "    \"\"\"\n",
        "    Long Format CSV 파일을 입력받아 FDC 분석을 자동으로 수행하는 메인 함수\n",
        "    \"\"\"\n",
        "    try:\n",
        "        from kneed import KneeLocator\n",
        "    except ImportError:\n",
        "        print(\"오류: 'kneed' 라이브러리가 설치되지 않았습니다. 'pip install kneed'를 실행해주세요.\")\n",
        "        return\n",
        "\n",
        "    print(\"\\n--- Step 1: 데이터 로드 및 컬럼 탐지 ---\")\n",
        "    try:\n",
        "        df_long = pd.read_csv(filepath)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"오류: '{filepath}' 파일을 찾을 수 없습니다.\")\n",
        "        return\n",
        "\n",
        "    wafer_id_col, step_id_col, sensor_cols = find_columns(df_long)\n",
        "\n",
        "    if not all([wafer_id_col, step_id_col, sensor_cols]):\n",
        "        print(\"오류: Wafer ID, Step ID, Sensor 컬럼을 모두 탐지해야 분석을 진행할 수 있습니다.\")\n",
        "        return\n",
        "\n",
        "    print(f\"ID 컬럼 탐지: Wafer='{wafer_id_col}', Step='{step_id_col}'\")\n",
        "    print(f\"총 {len(sensor_cols)}개의 센서 컬럼 탐지: {sensor_cols}\")\n",
        "\n",
        "    df_wide = extract_and_pivot_features(df_long, wafer_id_col, step_id_col, sensor_cols)\n",
        "    feature_cols = [col for col in df_wide.columns if col != wafer_id_col]\n",
        "\n",
        "    print(\"\\n--- Step 2: 자동 eps 추정 및 DBSCAN 군집화 ---\")\n",
        "    X = df_wide[feature_cols]\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    min_samples = 5\n",
        "    nearest_neighbors = NearestNeighbors(n_neighbors=min_samples)\n",
        "    neighbors = nearest_neighbors.fit(X_scaled)\n",
        "    distances, _ = neighbors.kneighbors(X_scaled)\n",
        "    sorted_distances = np.sort(distances[:, min_samples-1])\n",
        "\n",
        "    kneedle = KneeLocator(x=range(1, len(sorted_distances)+1), y=sorted_distances, S=1.0, curve=\"convex\", direction=\"increasing\")\n",
        "    optimal_eps = kneedle.elbow_y\n",
        "\n",
        "    if optimal_eps is None:\n",
        "        print(\"경고: 최적 eps 값을 찾지 못했습니다. 기본값으로 대체합니다.\")\n",
        "        optimal_eps = np.median(sorted_distances)\n",
        "\n",
        "    print(f\"자동으로 찾은 최적의 eps 값: {optimal_eps:.4f}\")\n",
        "\n",
        "    dbscan = DBSCAN(eps=optimal_eps, min_samples=min_samples)\n",
        "    clusters = dbscan.fit_predict(X_scaled)\n",
        "    df_wide['dbscan_cluster'] = clusters\n",
        "\n",
        "    cluster_counts = pd.Series(clusters).value_counts()\n",
        "    print(\"\\nDBSCAN 군집화 결과:\\n\", cluster_counts)\n",
        "\n",
        "    if -1 in cluster_counts.index and len(cluster_counts) > 1:\n",
        "        main_cluster_label = cluster_counts.drop(-1).idxmax()\n",
        "    else:\n",
        "        main_cluster_label = cluster_counts.idxmax()\n",
        "    print(f\"\\n가장 큰 군집(정상 상태로 추정): Cluster {main_cluster_label}\")\n",
        "\n",
        "    print(\"\\n--- Step 3: PCA 모델 학습 및 상태 인덱스 계산 ---\")\n",
        "    golden_df = df_wide[df_wide['dbscan_cluster'] == main_cluster_label].copy()\n",
        "\n",
        "    if golden_df.empty:\n",
        "        print(\"경고: 정상 군집이 없어 전체 데이터로 PCA 모델을 학습합니다.\")\n",
        "        scaler_pca = StandardScaler()\n",
        "        golden_features_scaled = scaler_pca.fit_transform(df_wide[feature_cols])\n",
        "    else:\n",
        "        print(f\"정상으로 식별된 데이터 개수: {len(golden_df)}\")\n",
        "        scaler_pca = StandardScaler()\n",
        "        golden_features_scaled = scaler_pca.fit_transform(golden_df[feature_cols])\n",
        "\n",
        "    pca_model = PCA(n_components=0.95)\n",
        "    pca_model.fit(golden_features_scaled)\n",
        "    print(f\"PCA 모델 학습 완료. 선택된 주성분 개수: {pca_model.n_components_}\")\n",
        "\n",
        "    all_features_scaled = scaler_pca.transform(df_wide[feature_cols])\n",
        "    all_pca_scores = pca_model.transform(all_features_scaled)\n",
        "\n",
        "    # --- [수정됨] Health Index 계산 방식을 절댓값으로 변경 ---\n",
        "    raw_health_index = all_pca_scores[:, 0]\n",
        "    df_wide['health_index'] = np.abs(raw_health_index)\n",
        "    print(\"Health Index 계산 완료 (절댓값 기준).\")\n",
        "\n",
        "    print(\"\\n--- Step 4: 최종 결과 시각화 ---\")\n",
        "    plt.figure(figsize=(16, 8))\n",
        "    sns.lineplot(x=wafer_id_col, y='health_index', data=df_wide, marker='o', color='gray', zorder=1, label='_nolegend_')\n",
        "    sns.scatterplot(x=wafer_id_col, y='health_index', data=df_wide, hue='dbscan_cluster', palette='viridis', s=80, zorder=2)\n",
        "\n",
        "    if not golden_df.empty:\n",
        "        normal_mean_index = df_wide[df_wide['dbscan_cluster'] == main_cluster_label]['health_index'].mean()\n",
        "        plt.axhline(y=normal_mean_index, color='dodgerblue', linestyle=':', linewidth=2, label=f'Normal Cluster Mean Index')\n",
        "\n",
        "    plt.title('Unsupervised FDC Health Index (Multi-Step)', fontsize=18)\n",
        "    plt.xlabel('Wafer ID', fontsize=12)\n",
        "    plt.ylabel('Health Index (Absolute PC1 Score)', fontsize=12) # Y축 라벨 변경\n",
        "    plt.legend(title='DBSCAN Cluster')\n",
        "    plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    diagnose_top_wafers_by_health_index(df_wide, golden_df, wafer_id_col, feature_cols, top_n=3)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    create_sample_long_format_csv(filepath='sample_fdc_long_data.csv')\n",
        "    csv_file_path = 'sample_fdc_long_data.csv'\n",
        "    analyze_fdc_from_csv(csv_file_path)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "0b_iaSHFOWRk"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}