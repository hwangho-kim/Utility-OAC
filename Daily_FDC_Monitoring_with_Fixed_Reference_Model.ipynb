{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hwangho-kim/Utility-OAC/blob/main/Daily_FDC_Monitoring_with_Fixed_Reference_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import joblib\n",
        "from datetime import date, timedelta\n",
        "\n",
        "# --- 그래프 한글 폰트 설정 ---\n",
        "try:\n",
        "    import koreanize_matplotlib\n",
        "except ImportError:\n",
        "    pass\n",
        "\n",
        "# --- 데이터 생성 함수들 ---\n",
        "def create_long_format_csv(filepath, num_wafers, anomaly_info=None):\n",
        "    \"\"\"\n",
        "    지정된 조건으로 Long Format CSV 데이터를 생성하는 범용 함수\n",
        "    \"\"\"\n",
        "    print(f\"'{filepath}' 이름으로 샘플 CSV 파일을 생성합니다...\")\n",
        "    num_steps = 10\n",
        "    num_sensors = 10\n",
        "    time_points = 10\n",
        "    np.random.seed(42)\n",
        "\n",
        "    records = []\n",
        "    for wafer_id in range(1, num_wafers + 1):\n",
        "        for step_id in range(1, num_steps + 1):\n",
        "            record_base = {'wafer_id': wafer_id, 'step_id': step_id}\n",
        "            for i in range(num_sensors):\n",
        "                sensor_name = f'Sensor_{chr(65+i)}'\n",
        "                record_base[sensor_name] = np.random.randn(time_points) * (i * 0.1 + 0.5) + (i * 5 + step_id)\n",
        "\n",
        "            # 이상 상태 또는 열화 주입\n",
        "            if anomaly_info:\n",
        "                if (anomaly_info.get('wafer_id') is None or wafer_id >= anomaly_info['wafer_id']) and \\\n",
        "                   (anomaly_info.get('step_id') is None or step_id == anomaly_info['step_id']):\n",
        "                    record_base['Sensor_C'] += anomaly_info.get('c_drift', 0)\n",
        "                    record_base['Sensor_G'] *= anomaly_info.get('g_noise', 1)\n",
        "\n",
        "            for t_idx in range(time_points):\n",
        "                row = {'wafer_id': record_base['wafer_id'], 'step_id': record_base['step_id'], 'time': t_idx}\n",
        "                for i in range(num_sensors):\n",
        "                    sensor_name = f'Sensor_{chr(65+i)}'\n",
        "                    row[sensor_name] = record_base[sensor_name][t_idx]\n",
        "                records.append(row)\n",
        "\n",
        "    df = pd.DataFrame(records)\n",
        "    df.to_csv(filepath, index=False)\n",
        "    print(\"샘플 파일 생성 완료.\")\n",
        "    return df\n",
        "\n",
        "# --- 특징 추출 및 컬럼 탐지 함수들 (이전과 동일) ---\n",
        "def find_columns(df):\n",
        "    id_candidates = {'wafer': ['wafer_id'], 'step': ['step_id'], 'time': ['time']}\n",
        "    detected_cols = {}\n",
        "    remaining_cols = list(df.columns)\n",
        "    for id_type, candidates in id_candidates.items():\n",
        "        found = False\n",
        "        for col in remaining_cols:\n",
        "            if col.lower() in candidates:\n",
        "                detected_cols[id_type] = col; remaining_cols.remove(col); found = True; break\n",
        "        if not found: detected_cols[id_type] = None\n",
        "    sensor_cols = [col for col in remaining_cols if pd.api.types.is_numeric_dtype(df[col])]\n",
        "    return detected_cols['wafer'], detected_cols['step'], sensor_cols\n",
        "\n",
        "def extract_and_pivot_features(df, wafer_id_col, step_id_col, sensor_cols):\n",
        "    def rms(x): return np.sqrt(np.mean(x**2))\n",
        "    agg_funcs = ['mean', 'std', 'max', 'min', 'median', 'skew', pd.Series.kurt, rms]\n",
        "    features = df.groupby([wafer_id_col, step_id_col])[sensor_cols].agg(agg_funcs).reset_index()\n",
        "    new_cols = [wafer_id_col, step_id_col]\n",
        "    for col_level0, col_level1 in features.columns[2:]:\n",
        "        func_name = col_level1 if isinstance(col_level1, str) else col_level1.__name__\n",
        "        new_cols.append(f\"{col_level0}_{func_name}\")\n",
        "    features.columns = new_cols\n",
        "    feature_pivot = features.pivot(index=wafer_id_col, columns=step_id_col)\n",
        "    feature_pivot.columns = [f\"S{int(col[1])}_{col[0]}\" for col in feature_pivot.columns.values]\n",
        "    feature_pivot.fillna(0, inplace=True)\n",
        "    return feature_pivot.reset_index()\n",
        "\n",
        "# ======================================================================================\n",
        "# Phase 1: 기준 모델 생성 (최초 1회 실행)\n",
        "# ======================================================================================\n",
        "def train_and_save_reference_model(golden_data_path='golden_data.csv'):\n",
        "    print(\"\\n\" + \"=\"*25 + \" Phase 1: 기준 모델 생성 \" + \"=\"*25)\n",
        "\n",
        "    # 1. 골든 데이터셋 생성\n",
        "    create_long_format_csv(golden_data_path, num_wafers=100)\n",
        "    df_long = pd.read_csv(golden_data_path)\n",
        "\n",
        "    # 2. 특징 추출 및 변환\n",
        "    wafer_id_col, step_id_col, sensor_cols = find_columns(df_long)\n",
        "    df_wide = extract_and_pivot_features(df_long, wafer_id_col, step_id_col, sensor_cols)\n",
        "    feature_cols = [col for col in df_wide.columns if col != wafer_id_col]\n",
        "\n",
        "    # 3. DBSCAN으로 정상 군집 찾기\n",
        "    scaler_dbscan = StandardScaler()\n",
        "    X_scaled = scaler_dbscan.fit_transform(df_wide[feature_cols])\n",
        "\n",
        "    try: from kneed import KneeLocator\n",
        "    except ImportError: print(\"오류: 'kneed' 라이브러리가 필요합니다. 'pip install kneed'를 실행해주세요.\"); return\n",
        "\n",
        "    nearest_neighbors = NearestNeighbors(n_neighbors=5).fit(X_scaled)\n",
        "    distances, _ = nearest_neighbors.kneighbors(X_scaled)\n",
        "    sorted_distances = np.sort(distances[:, 4])\n",
        "    kneedle = KneeLocator(x=range(1, len(sorted_distances)+1), y=sorted_distances, S=1.0, curve=\"convex\", direction=\"increasing\")\n",
        "    optimal_eps = kneedle.elbow_y or np.median(sorted_distances)\n",
        "\n",
        "    dbscan = DBSCAN(eps=optimal_eps, min_samples=5)\n",
        "    clusters = dbscan.fit_predict(X_scaled)\n",
        "    df_wide['dbscan_cluster'] = clusters\n",
        "\n",
        "    main_cluster_label = pd.Series(clusters).value_counts().idxmax()\n",
        "    golden_df = df_wide[df_wide['dbscan_cluster'] == main_cluster_label].copy()\n",
        "    print(f\"골든 데이터셋에서 {len(golden_df)}개의 정상 Wafer를 식별했습니다.\")\n",
        "\n",
        "    # 4. 정상 군집 데이터로 기준 모델(스케일러, PCA) 학습\n",
        "    scaler_pca = StandardScaler()\n",
        "    golden_features_scaled = scaler_pca.fit_transform(golden_df[feature_cols])\n",
        "\n",
        "    pca_model = PCA(n_components=0.95)\n",
        "    pca_model.fit(golden_features_scaled)\n",
        "\n",
        "    # 5. 학습된 모델 파일로 저장\n",
        "    joblib.dump(scaler_pca, 'scaler.joblib')\n",
        "    joblib.dump(pca_model, 'pca.joblib')\n",
        "    joblib.dump(feature_cols, 'feature_cols.joblib') # 특징 이름 순서도 저장\n",
        "\n",
        "    print(\"\\n기준 모델 생성 및 저장 완료: scaler.joblib, pca.joblib, feature_cols.joblib\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "# ======================================================================================\n",
        "# Phase 2: 일일 모니터링 (매일 반복 실행)\n",
        "# ======================================================================================\n",
        "def run_daily_monitoring(days_to_monitor=10):\n",
        "    print(\"\\n\" + \"=\"*25 + \" Phase 2: 일일 모니터링 시작 \" + \"=\"*25)\n",
        "\n",
        "    # 1. 저장된 기준 모델 불러오기\n",
        "    try:\n",
        "        scaler_pca = joblib.load('scaler.joblib')\n",
        "        pca_model = joblib.load('pca.joblib')\n",
        "        feature_cols = joblib.load('feature_cols.joblib')\n",
        "        print(\"저장된 기준 모델(scaler, pca, feature_cols)을 성공적으로 불러왔습니다.\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"오류: 기준 모델 파일(.joblib)을 찾을 수 없습니다. Phase 1을 먼저 실행해주세요.\")\n",
        "        return\n",
        "\n",
        "    daily_log = []\n",
        "    start_date = date.today()\n",
        "\n",
        "    for day in range(days_to_monitor):\n",
        "        current_date = start_date + timedelta(days=day)\n",
        "        print(f\"\\n--- {current_date} 데이터 모니터링 ---\")\n",
        "\n",
        "        # 2. 일일 데이터 생성 (시간이 지남에 따라 미세하게 열화)\n",
        "        daily_data_path = f'daily_data_day_{day+1}.csv'\n",
        "        degradation_info = {'c_drift': day * 0.1, 'g_noise': 1 + day * 0.02}\n",
        "        create_long_format_csv(daily_data_path, num_wafers=20, anomaly_info=degradation_info)\n",
        "        df_long_daily = pd.read_csv(daily_data_path)\n",
        "\n",
        "        # 3. 특징 추출 및 변환\n",
        "        wafer_id_col, step_id_col, sensor_cols = find_columns(df_long_daily)\n",
        "        df_wide_daily = extract_and_pivot_features(df_long_daily, wafer_id_col, step_id_col, sensor_cols)\n",
        "\n",
        "        # 저장된 특징 순서와 동일하게 맞춤\n",
        "        df_wide_daily = df_wide_daily[df_wide_daily.columns.intersection([wafer_id_col] + feature_cols)]\n",
        "\n",
        "        # 4. 불러온 기준 모델로 Health Index 계산 (절대 학습(fit)하지 않음!)\n",
        "        daily_features_scaled = scaler_pca.transform(df_wide_daily[feature_cols])\n",
        "        daily_pca_scores = pca_model.transform(daily_features_scaled)\n",
        "\n",
        "        health_indexes = np.abs(daily_pca_scores[:, 0])\n",
        "\n",
        "        # 5. Daily Index 계산 및 기록\n",
        "        daily_index = np.mean(health_indexes)\n",
        "        print(f\"{current_date}의 Daily Index: {daily_index:.4f}\")\n",
        "        daily_log.append({'date': current_date, 'daily_index': daily_index})\n",
        "\n",
        "    # 6. Daily Index 로그 저장 및 시각화\n",
        "    log_df = pd.DataFrame(daily_log)\n",
        "    log_df.to_csv('daily_health_log.csv', index=False)\n",
        "    print(\"\\n일일 모니터링 로그 저장 완료: daily_health_log.csv\")\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    sns.lineplot(x='date', y='daily_index', data=log_df, marker='o')\n",
        "    plt.title('Daily Equipment Health Index Trend', fontsize=16)\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Daily Health Index (Lower is Better)')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.grid(True, linestyle='--')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Phase 1: 기준이 되는 모델을 생성하고 파일로 저장합니다.\n",
        "    # 이 부분은 실제 운영 시, 장비가 가장 안정적일 때 단 한 번만 실행합니다.\n",
        "    train_and_save_reference_model()\n",
        "\n",
        "    # Phase 2: 저장된 모델을 불러와 매일 모니터링을 수행합니다.\n",
        "    # 이 부분은 매일 새로운 데이터에 대해 반복적으로 실행됩니다.\n",
        "    run_daily_monitoring(days_to_monitor=10)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "lEokUarTYDiu"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}