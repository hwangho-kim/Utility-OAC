{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hwangho-kim/Utility-OAC/blob/main/Daily_FDC_Monitoring_(PCA_%2B_Mahalanobis).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import joblib\n",
        "from datetime import date, timedelta\n",
        "from scipy.spatial.distance import mahalanobis\n",
        "\n",
        "# --- 그래프 한글 폰트 설정 ---\n",
        "try:\n",
        "    import koreanize_matplotlib\n",
        "except ImportError:\n",
        "    pass\n",
        "\n",
        "# --- 데이터 생성 및 특징 추출 함수들 (이전과 동일) ---\n",
        "def create_long_format_csv(filepath, num_wafers, anomaly_info=None):\n",
        "    print(f\"'{filepath}' 이름으로 샘플 CSV 파일을 생성합니다...\")\n",
        "    num_steps, num_sensors, time_points = 10, 10, 10\n",
        "    np.random.seed(42)\n",
        "    records = []\n",
        "    for wafer_id in range(1, num_wafers + 1):\n",
        "        for step_id in range(1, num_steps + 1):\n",
        "            record_base = {'wafer_id': wafer_id, 'step_id': step_id}\n",
        "            for i in range(num_sensors):\n",
        "                sensor_name = f'Sensor_{chr(65+i)}'\n",
        "                record_base[sensor_name] = np.random.randn(time_points) * (i * 0.1 + 0.5) + (i * 5 + step_id)\n",
        "            if anomaly_info:\n",
        "                if (anomaly_info.get('wafer_id') is None or wafer_id >= anomaly_info['wafer_id']) and \\\n",
        "                   (anomaly_info.get('step_id') is None or step_id == anomaly_info['step_id']):\n",
        "                    record_base['Sensor_C'] += anomaly_info.get('c_drift', 0)\n",
        "                    record_base['Sensor_G'] *= anomaly_info.get('g_noise', 1)\n",
        "            for t_idx in range(time_points):\n",
        "                row = {**record_base, 'time': t_idx}\n",
        "                for i in range(num_sensors):\n",
        "                    sensor_name = f'Sensor_{chr(65+i)}'\n",
        "                    row[sensor_name] = record_base[sensor_name][t_idx]\n",
        "                records.append(row)\n",
        "    df = pd.DataFrame(records)\n",
        "    df.to_csv(filepath, index=False)\n",
        "    print(\"샘플 파일 생성 완료.\")\n",
        "    return df\n",
        "\n",
        "def find_columns(df):\n",
        "    id_candidates = {'wafer': ['wafer_id'], 'step': ['step_id'], 'time': ['time']}\n",
        "    detected_cols = {}\n",
        "    remaining_cols = list(df.columns)\n",
        "    for id_type, candidates in id_candidates.items():\n",
        "        found = False\n",
        "        for col in remaining_cols:\n",
        "            if col.lower() in candidates:\n",
        "                detected_cols[id_type] = col; remaining_cols.remove(col); found = True; break\n",
        "        if not found: detected_cols[id_type] = None\n",
        "    sensor_cols = [col for col in remaining_cols if pd.api.types.is_numeric_dtype(df[col])]\n",
        "    return detected_cols['wafer'], detected_cols['step'], sensor_cols\n",
        "\n",
        "def extract_and_pivot_features(df, wafer_id_col, step_id_col, sensor_cols):\n",
        "    def rms(x): return np.sqrt(np.mean(x**2))\n",
        "    agg_funcs = ['mean', 'std', 'max', 'min', 'median', 'skew', pd.Series.kurt, rms]\n",
        "    features = df.groupby([wafer_id_col, step_id_col])[sensor_cols].agg(agg_funcs).reset_index()\n",
        "    new_cols = [wafer_id_col, step_id_col]\n",
        "    for col_level0, col_level1 in features.columns[2:]:\n",
        "        func_name = col_level1 if isinstance(col_level1, str) else col_level1.__name__\n",
        "        new_cols.append(f\"{col_level0}_{func_name}\")\n",
        "    features.columns = new_cols\n",
        "    feature_pivot = features.pivot(index=wafer_id_col, columns=step_id_col)\n",
        "    feature_pivot.columns = [f\"S{int(col[1])}_{col[0]}\" for col in feature_pivot.columns.values]\n",
        "    feature_pivot.fillna(0, inplace=True)\n",
        "    return feature_pivot.reset_index()\n",
        "\n",
        "# --- [신규] Wafer 진단 리포트 함수 ---\n",
        "def diagnose_daily_wafers(df_daily, normal_stats, wafer_id_col, feature_cols, top_n=3):\n",
        "    \"\"\"\n",
        "    그날의 Wafer 중 Health Index가 높은 상위 Wafer에 대해 원인 분석을 수행합니다.\n",
        "    \"\"\"\n",
        "    print(f\"\\n--- 당일 Health Index 상위 {top_n}개 Wafer 상세 진단 ---\")\n",
        "\n",
        "    df_sorted = df_daily.sort_values(by='health_index', ascending=False)\n",
        "\n",
        "    for i, row in enumerate(df_sorted.head(top_n).itertuples()):\n",
        "        wafer_id = getattr(row, wafer_id_col)\n",
        "\n",
        "        z_scores = {}\n",
        "        for feature in feature_cols:\n",
        "            val = getattr(row, feature)\n",
        "            mean = normal_stats.loc['mean', feature]\n",
        "            std = normal_stats.loc['std', feature]\n",
        "            if std > 1e-6:\n",
        "                z_scores[feature] = (val - mean) / std\n",
        "\n",
        "        sorted_features = sorted(z_scores.items(), key=lambda item: abs(item[1]), reverse=True)\n",
        "\n",
        "        print(f\"\\n[{i+1}] Wafer ID: {wafer_id} (Health Index: {row.health_index:.2f})\")\n",
        "        print(\"  > 상위 원인 특징 (Z-score 기준):\")\n",
        "        for feature, score in sorted_features[:3]:\n",
        "            parts = feature.split('_')\n",
        "            step_info = parts[0].replace('S', '')\n",
        "            stat_info = parts[-1]\n",
        "            sensor_info = '_'.join(parts[1:-1])\n",
        "            print(f\"    - Step: {step_info}, Sensor: {sensor_info}, Statistic: {stat_info}, Z-score: {score:.2f}\")\n",
        "\n",
        "# ======================================================================================\n",
        "# Phase 1: 기준 모델 생성 (최초 1회 실행)\n",
        "# ======================================================================================\n",
        "def train_and_save_reference_model(golden_data_path='golden_data.csv'):\n",
        "    print(\"\\n\" + \"=\"*25 + \" Phase 1: 기준 모델 생성 \" + \"=\"*25)\n",
        "\n",
        "    create_long_format_csv(golden_data_path, num_wafers=100)\n",
        "    df_long = pd.read_csv(golden_data_path)\n",
        "    wafer_id_col, step_id_col, sensor_cols = find_columns(df_long)\n",
        "    df_wide = extract_and_pivot_features(df_long, wafer_id_col, step_id_col, sensor_cols)\n",
        "    feature_cols = [col for col in df_wide.columns if col != wafer_id_col]\n",
        "\n",
        "    scaler_dbscan = StandardScaler()\n",
        "    X_scaled = scaler_dbscan.fit_transform(df_wide[feature_cols])\n",
        "\n",
        "    try: from kneed import KneeLocator\n",
        "    except ImportError: print(\"오류: 'kneed' 라이브러리가 필요합니다. 'pip install kneed'를 실행해주세요.\"); return\n",
        "\n",
        "    nearest_neighbors = NearestNeighbors(n_neighbors=5).fit(X_scaled)\n",
        "    distances, _ = nearest_neighbors.kneighbors(X_scaled)\n",
        "    sorted_distances = np.sort(distances[:, 4])\n",
        "    kneedle = KneeLocator(x=range(1, len(sorted_distances)+1), y=sorted_distances, S=1.0, curve=\"convex\", direction=\"increasing\")\n",
        "    optimal_eps = kneedle.elbow_y or np.median(sorted_distances)\n",
        "\n",
        "    dbscan = DBSCAN(eps=optimal_eps, min_samples=5)\n",
        "    clusters = dbscan.fit_predict(X_scaled)\n",
        "    df_wide['dbscan_cluster'] = clusters\n",
        "\n",
        "    main_cluster_label = pd.Series(clusters).value_counts().idxmax()\n",
        "    golden_df = df_wide[df_wide['dbscan_cluster'] == main_cluster_label].copy()\n",
        "    print(f\"골든 데이터셋에서 {len(golden_df)}개의 정상 Wafer를 식별했습니다.\")\n",
        "\n",
        "    scaler_final = StandardScaler()\n",
        "    golden_features_scaled = scaler_final.fit_transform(golden_df[feature_cols])\n",
        "\n",
        "    pca_model = PCA(n_components=0.95)\n",
        "    golden_features_pca = pca_model.fit_transform(golden_features_scaled)\n",
        "    print(f\"차원 축소 완료: {len(feature_cols)}개 특징 -> {pca_model.n_components_}개 주성분\")\n",
        "\n",
        "    mean_vector_pca = np.mean(golden_features_pca, axis=0)\n",
        "    covariance_matrix_pca = np.cov(golden_features_pca, rowvar=False)\n",
        "    inv_covariance_matrix_pca = np.linalg.inv(covariance_matrix_pca)\n",
        "\n",
        "    golden_distances = [mahalanobis(row, mean_vector_pca, inv_covariance_matrix_pca) for row in golden_features_pca]\n",
        "    health_index_baseline = np.percentile(golden_distances, 99)\n",
        "    scaling_factor = health_index_baseline / 10.0 + 1e-9\n",
        "    print(f\"\\n정규화 기준값(99th percentile Mahalanobis Distance in PCA space): {health_index_baseline:.4f}\")\n",
        "    print(f\"최종 인덱스용 스케일링 팩터: {scaling_factor:.4f}\")\n",
        "\n",
        "    # [수정됨] 진단을 위한 정상 상태 통계량 저장\n",
        "    normal_stats = golden_df[feature_cols].agg(['mean', 'std'])\n",
        "\n",
        "    joblib.dump(scaler_final, 'scaler_final.joblib')\n",
        "    joblib.dump(pca_model, 'pca_model.joblib')\n",
        "    joblib.dump(mean_vector_pca, 'mean_vector_pca.joblib')\n",
        "    joblib.dump(inv_covariance_matrix_pca, 'inv_cov_matrix_pca.joblib')\n",
        "    joblib.dump(feature_cols, 'feature_cols.joblib')\n",
        "    joblib.dump(scaling_factor, 'scaling_factor_final.joblib')\n",
        "    joblib.dump(normal_stats, 'normal_stats.joblib') # 정상 통계량 저장\n",
        "\n",
        "    print(\"\\n기준 모델 생성 및 저장 완료.\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "# ======================================================================================\n",
        "# Phase 2: 일일 모니터링 (매일 반복 실행)\n",
        "# ======================================================================================\n",
        "def run_daily_monitoring(days_to_monitor=10):\n",
        "    print(\"\\n\" + \"=\"*25 + \" Phase 2: 일일 모니터링 시작 \" + \"=\"*25)\n",
        "\n",
        "    try:\n",
        "        scaler_final = joblib.load('scaler_final.joblib')\n",
        "        pca_model = joblib.load('pca_model.joblib')\n",
        "        mean_vector_pca = joblib.load('mean_vector_pca.joblib')\n",
        "        inv_covariance_matrix_pca = joblib.load('inv_cov_matrix_pca.joblib')\n",
        "        feature_cols = joblib.load('feature_cols.joblib')\n",
        "        scaling_factor = joblib.load('scaling_factor_final.joblib')\n",
        "        normal_stats = joblib.load('normal_stats.joblib') # 정상 통계량 로드\n",
        "        print(\"저장된 기준 모델 및 정상 통계량을 성공적으로 불러왔습니다.\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"오류: 기준 모델 파일(.joblib)을 찾을 수 없습니다. Phase 1을 먼저 실행해주세요.\")\n",
        "        return\n",
        "\n",
        "    daily_log = []\n",
        "    start_date = date.today()\n",
        "\n",
        "    for day in range(days_to_monitor):\n",
        "        current_date = start_date + timedelta(days=day)\n",
        "        print(f\"\\n--- {current_date} 데이터 모니터링 ---\")\n",
        "\n",
        "        daily_data_path = f'daily_data_day_{day+1}.csv'\n",
        "        degradation_info = {'c_drift': day * 0.1, 'g_noise': 1 + day * 0.02}\n",
        "        create_long_format_csv(daily_data_path, num_wafers=20, anomaly_info=degradation_info)\n",
        "        df_long_daily = pd.read_csv(daily_data_path)\n",
        "\n",
        "        wafer_id_col, step_id_col, sensor_cols = find_columns(df_long_daily)\n",
        "        df_wide_daily = extract_and_pivot_features(df_long_daily, wafer_id_col, step_id_col, sensor_cols)\n",
        "        df_wide_daily = df_wide_daily[df_wide_daily.columns.intersection([wafer_id_col] + feature_cols)]\n",
        "\n",
        "        daily_features_scaled = scaler_final.transform(df_wide_daily[feature_cols])\n",
        "        daily_features_pca = pca_model.transform(daily_features_scaled)\n",
        "\n",
        "        raw_health_indexes = [mahalanobis(row, mean_vector_pca, inv_covariance_matrix_pca) for row in daily_features_pca]\n",
        "        df_wide_daily['health_index'] = raw_health_indexes # 진단을 위해 정규화 전 인덱스 추가\n",
        "\n",
        "        raw_daily_index = np.mean(raw_health_indexes)\n",
        "        normalized_daily_index = raw_daily_index / scaling_factor\n",
        "\n",
        "        print(f\"{current_date}의 정규화된 Daily Index: {normalized_daily_index:.4f}\")\n",
        "        daily_log.append({'date': current_date, 'daily_index': normalized_daily_index})\n",
        "\n",
        "        # --- [추가됨] 당일 Wafer 진단 리포트 호출 ---\n",
        "        diagnose_daily_wafers(df_wide_daily, normal_stats, wafer_id_col, feature_cols, top_n=3)\n",
        "\n",
        "    log_df = pd.DataFrame(daily_log)\n",
        "    log_df.to_csv('daily_health_log.csv', index=False)\n",
        "    print(\"\\n일일 모니터링 로그 저장 완료: daily_health_log.csv\")\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    sns.lineplot(x='date', y='daily_index', data=log_df, marker='o')\n",
        "    plt.axhline(y=10, color='r', linestyle='--', label='Warning Threshold (10)')\n",
        "    plt.title('Daily Equipment Health Index Trend (PCA + Mahalanobis)', fontsize=16)\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Normalized Daily Index (Normal < 10)')\n",
        "    plt.legend()\n",
        "    plt.grid(True, linestyle='--')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    train_and_save_reference_model()\n",
        "    run_daily_monitoring(days_to_monitor=10)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "POn0Rz8i7oc6"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}